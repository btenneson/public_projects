Link to Kaggle Notebook housing Data: [Heart Disease](https://www.kaggle.com/code/alexteboul/heart-disease-health-indicators-dataset-notebook)
# In this database, we have collected information about heart health as a function of several variables.
import pandas as pd
import warnings
warnings.filterwarnings('ignore')
import numpy as np
import matplotlib.pyplot as plt

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score
from sklearn.preprocessing import StandardScaler
from sklearn.base import clone


# Load heart disease dataset
url = "https://raw.githubusercontent.com/btenneson/public_projects/refs/heads/main/predator/heart_disease_health_indicators_BRFSS2015.csv"
data = pd.read_csv(url)
print(data.head())


# Save a local copy
filename = "113_kaggle_Heart_Disease_Health_Indicators_Dataset.csv"
data.to_csv(filename, index=False)
print(f"File downloaded and saved as '{filename}'")


# Split features and target
X = data.iloc[:, 1:]
y = data.iloc[:, 0]


# Show class distribution
print("\nTarget class distribution:")
print(y.value_counts(normalize=True))


# Train-test split (80% train, 20% test), stratified
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)


# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


# Custom scorer: maximize best F1 across thresholds
def f1_threshold_tuner(estimator, X_val, y_val):
    model = clone(estimator)
    model.fit(X_train_scaled, y_train)  # Fit on entire training data
    y_pred_proba = model.predict_proba(X_val)[:, 1]
    thresholds = np.linspace(0, 1, 100)
    best_f1 = 0
    for d in thresholds:
        y_pred = (y_pred_proba >= d).astype(int)
        f1 = f1_score(y_val, y_pred)
        if f1 > best_f1:
            best_f1 = f1
    return best_f1


# Initialize model
gb_model = GradientBoostingClassifier(random_state=42)


# Define grid
param_grid = {
    'n_estimators': [75,125],
    'max_depth': [2,7,8,10],
    'learning_rate': [0.1, 0.01,.001],
    'subsample': [0.8]
}


# CV
cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=1, random_state=42)


# Metrics for evaluation
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1': 'f1',
    'auc': 'roc_auc',
    'f1_threshold_tuned': f1_threshold_tuner
}


# Grid search with threshold-tuned F1 as refit metric
grid_search = GridSearchCV(
    estimator=gb_model,
    param_grid=param_grid,
    scoring=scoring,
    refit='f1_threshold_tuned',
    cv=cv,
    verbose=1,
    n_jobs=-1
)


# Train model
grid_search.fit(X_train_scaled, y_train)


# Best parameters
best_params = grid_search.best_params_
print("Best parameters found:", best_params)


# Best model
best_model = grid_search.best_estimator_


# Feature importances
feature_importances = best_model.feature_importances_
feature_importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': feature_importances
}).sort_values(by='Importance', ascending=False)


# Plot feature importances
import numpy as np
indices = np.argsort(feature_importances)[::-1]
plt.figure(figsize=(12, 8))
plt.bar(range(X_train.shape[1]), feature_importances[indices], align='center')
plt.xticks(range(X_train.shape[1]), X.columns[indices], rotation=90)
plt.xlabel('Features')
plt.ylabel('Importance')
plt.tight_layout()
plt.show()


# Save feature importances
feature_importance_df.to_csv("feature_importances.csv", index=False)
print("Feature importances saved to 'feature_importances.csv'")


# Predict probabilities
y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]


# Threshold sweep to optimize F1
thresholds = np.linspace(0, 1, 100)
f1_scores = []
correlations = []


for d in thresholds:
    y_pred = (y_pred_proba >= d).astype(int)
    f1_scores.append(f1_score(y_test, y_pred))
    # You can still check correlation if you want
    corr = np.corrcoef(y_pred, y_test)[0, 1]
    correlations.append(corr)


# Find best thresholds
optimal_idx_f1 = np.argmax(f1_scores)
optimal_idx_corr = np.argmax(correlations)
best_d_f1 = thresholds[optimal_idx_f1]
best_d_corr = thresholds[optimal_idx_corr]


print(f"Optimal threshold for F1 score: {best_d_f1:.4f} (F1 = {f1_scores[optimal_idx_f1]:.4f})")
print(f"Optimal threshold for Correlation: {best_d_corr:.4f} (Correlation = {correlations[optimal_idx_corr]:.4f})")


# Plot F1 and Correlation vs threshold
plt.figure(figsize=(10, 6))
plt.plot(thresholds, f1_scores, label='F1 Score')
plt.plot(thresholds, correlations, label='Correlation')
plt.axvline(x=best_d_f1, color='blue', linestyle='--', label=f'Best F1 Threshold ({best_d_f1:.2f})')
plt.axvline(x=best_d_corr, color='orange', linestyle='--', label=f'Best Corr Threshold ({best_d_corr:.2f})')
plt.xlabel("Threshold (d)")
plt.ylabel("Score")
plt.title("F1 and Correlation vs Threshold")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


# Use best F1 threshold to make final predictions
y_pred = (y_pred_proba >= best_d_f1).astype(int)


# Classification report
print("\nClassification Report (Threshold = Best F1):")
print(classification_report(y_test, y_pred))


# Final metrics
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"Precision: {precision_score(y_test, y_pred):.4f}")
print(f"Recall: {recall_score(y_test, y_pred):.4f}")
print(f"F1 Score: {f1_score(y_test, y_pred):.4f}")
print(f"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}")
